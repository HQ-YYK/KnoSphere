from typing import TypedDict, Annotated, List, Optional
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage
from services.tools import get_tool_manager
from langgraph.prebuilt import ToolNode
from langchain_core.tools import BaseTool
from pydantic import BaseModel, Field
from services.search import secure_hybrid_search
from services.llm import get_llm_service
from sqlmodel import Session
from datetime import datetime
import json
from core.auth import get_current_user

# ==================== çŠ¶æ€å®šä¹‰ ====================

class AgentState(TypedDict):
    """ä»£ç†çŠ¶æ€ - åŒ…å«å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹"""
    # å¯¹è¯å†å²
    messages: Annotated[List[BaseMessage], add_messages]
    # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    documents: List[dict]
    # æœ€ç»ˆç”Ÿæˆå†…å®¹
    generation: str
    # å½“å‰æ‰§è¡ŒèŠ‚ç‚¹
    current_node: str
    # èŠ‚ç‚¹æ‰§è¡Œå†å²
    node_history: List[dict]
    # æ‰§è¡Œå¼€å§‹æ—¶é—´
    start_time: datetime
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str]
    # é‡è¯•æ¬¡æ•°
    retry_count: int
    # æ˜¯å¦ç›¸å…³
    is_relevant: Optional[bool]
    # æ·»åŠ å·¥å…·ç›¸å…³å­—æ®µ
    tool_calls: List[dict]  # å·¥å…·è°ƒç”¨è®°å½•
    tool_results: List[dict]  # å·¥å…·æ‰§è¡Œç»“æœ
    should_use_tools: bool  # æ˜¯å¦åº”è¯¥ä½¿ç”¨å·¥å…·

# ==================== æ¨¡å‹å®šä¹‰ ====================

class DocumentRelevance(BaseModel):
    """æ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°æ¨¡å‹"""
    binary_score: str = Field(
        description="æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦ä¸é—®é¢˜ç›¸å…³? è¿”å› 'yes' æˆ– 'no'",
        examples=["yes", "no"]
    )
    confidence: float = Field(
        description="è¯„ä¼°ç½®ä¿¡åº¦ (0.0-1.0)",
        ge=0.0,
        le=1.0
    )
    reason: str = Field(
        description="è¯„ä¼°ç†ç”±",
        examples=["æ–‡æ¡£å†…å®¹ç›´æ¥å›ç­”äº†ç”¨æˆ·é—®é¢˜", "æ–‡æ¡£å†…å®¹ä¸é—®é¢˜æ— å…³"]
    )

class QueryRewrite(BaseModel):
    """æŸ¥è¯¢é‡å†™æ¨¡å‹"""
    rewritten_query: str = Field(
        description="é‡å†™åçš„æŸ¥è¯¢è¯­å¥",
        examples=["äººå·¥æ™ºèƒ½çš„åŸºæœ¬åŸç†å’Œåº”ç”¨", "æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•æœ‰å“ªäº›"]
    )
    improvement: str = Field(
        description="é‡å†™æ”¹è¿›ç‚¹",
        examples=["æ›´å…·ä½“", "æ›´ä¸“ä¸š", "æ›´æ¸…æ™°"]
    )

# ==================== èŠ‚ç‚¹å®ç° ====================

async def start_node(state: AgentState) -> dict:
    """å¼€å§‹èŠ‚ç‚¹ - åˆå§‹åŒ–çŠ¶æ€"""
    return {
        "current_node": "start",
        "start_time": datetime.now(),
        "node_history": [{
            "node": "start",
            "timestamp": datetime.now().isoformat(),
            "status": "success"
        }]
    }

async def retrieve_node(state: AgentState, config: dict) -> dict:
    """æ£€ç´¢èŠ‚ç‚¹ - æ™ºèƒ½æ£€ç´¢æ–‡æ¡£"""
    try:
        db = config.get("db")
        if not db:
            raise ValueError("æ•°æ®åº“ä¼šè¯æœªæä¾›")
        
        user_id = config.get("user_id", "")
        
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        messages = state.get("messages", [])
        if not messages:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯"}
        
        last_message = messages[-1]
        query = getattr(last_message, 'content', str(last_message))
        
        # æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´æ£€ç´¢å‚æ•°
        query_len = len(query)
        top_k = 15 if query_len > 30 else 10
        final_k = 5 if query_len > 30 else 3
        
        # æ‰§è¡Œæ··åˆæ£€ç´¢
        print(f"ğŸ” æ‰§è¡Œæ™ºèƒ½æ£€ç´¢: {query[:50]}...")
        
        documents = await secure_hybrid_search(query, db, user_id, top_k=top_k, final_k=final_k)
        
        return {
            "documents": documents,
            "current_node": "retrieve",
            "node_history": state.get("node_history", []) + [{
                "node": "retrieve",
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "documents_count": len(documents),
                "query": query[:100]
            }]
        }
    except Exception as e:
        print(f"âŒ æ£€ç´¢èŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "error": f"æ£€ç´¢å¤±è´¥: {str(e)}",
            "current_node": "retrieve",
            "node_history": state.get("node_history", []) + [{
                "node": "retrieve",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }

async def grade_node(state: AgentState, config: dict) -> dict:
    """è¯„ä¼°èŠ‚ç‚¹ - è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§"""
    try:
        documents = state.get("documents", [])
        messages = state.get("messages", [])
        
        if not documents:
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ï¼Œç›´æ¥æ ‡è®°ä¸ºä¸ç›¸å…³
            return {
                "is_relevant": False,
                "current_node": "grade",
                "node_history": state.get("node_history", []) + [{
                    "node": "grade",
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "assessment": "no_documents",
                    "is_relevant": False
                }]
            }
        
        # è·å–ç”¨æˆ·æŸ¥è¯¢
        last_message = messages[-1]
        query = getattr(last_message, 'content', str(last_message))
        
        # å‡†å¤‡æ–‡æ¡£å†…å®¹ç”¨äºè¯„ä¼°
        doc_contents = [doc.get("content", "")[:500] for doc in documents[:3]]
        context = "\n\n".join([f"æ–‡æ¡£{i+1}: {content}" for i, content in enumerate(doc_contents)])
        
        # ä½¿ç”¨ LLM è¯„ä¼°ç›¸å…³æ€§
        llm_service = get_llm_service()
        
        # æ„å»ºè¯„ä¼°æç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£æ˜¯å¦èƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {query}

æ£€ç´¢åˆ°çš„æ–‡æ¡£:
{context}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœ:
1. binary_score: å¦‚æœæ–‡æ¡£å†…å®¹èƒ½å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¿”å›'yes'ï¼Œå¦åˆ™è¿”å›'no'
2. confidence: è¯„ä¼°ç½®ä¿¡åº¦ (0.0-1.0)
3. reason: ç®€è¦è¯´æ˜è¯„ä¼°ç†ç”±

æ³¨æ„: å³ä½¿æ–‡æ¡£å†…å®¹ä¸å®Œå…¨åŒ¹é…ï¼Œä½†å¦‚æœæ˜¯ç›¸å…³ä¸»é¢˜ï¼Œä¹Ÿåº”è€ƒè™‘ä¸ºç›¸å…³ã€‚"""

        # æ”¶é›†è¯„ä¼°å“åº”
        full_response = ""
        async for chunk in llm_service.stream_response(system_prompt, "è¯·è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§"):
            full_response += chunk
        
        # è§£æè¯„ä¼°ç»“æœ (ç®€åŒ–è§£æï¼Œå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º)
        is_relevant = "yes" in full_response.lower() or "ç›¸å…³" in full_response
        
        return {
            "is_relevant": is_relevant,
            "current_node": "grade",
            "node_history": state.get("node_history", []) + [{
                "node": "grade",
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "assessment": full_response[:200],
                "is_relevant": is_relevant,
                "confidence": 0.8 if is_relevant else 0.3
            }]
        }
    except Exception as e:
        print(f"âŒ è¯„ä¼°èŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "error": f"è¯„ä¼°å¤±è´¥: {str(e)}",
            "current_node": "grade",
            "node_history": state.get("node_history", []) + [{
                "node": "grade",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }

async def rewrite_node(state: AgentState, config: dict) -> dict:
    """é‡å†™èŠ‚ç‚¹ - ä¼˜åŒ–æŸ¥è¯¢è¯­å¥"""
    try:
        messages = state.get("messages", [])
        last_message = messages[-1]
        original_query = getattr(last_message, 'content', str(last_message))
        
        # ä½¿ç”¨ LLM é‡å†™æŸ¥è¯¢
        llm_service = get_llm_service()
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·é‡å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ–‡æ¡£æ£€ç´¢ã€‚

åŸå§‹æŸ¥è¯¢: {original_query}

é‡å†™è¦æ±‚:
1. ä¿æŒåŸæ„ï¼Œä½†è¡¨è¾¾æ›´æ¸…æ™°
2. å¦‚æœæ˜¯æ¨¡ç³ŠæŸ¥è¯¢ï¼Œå°è¯•ä½¿å…¶æ›´å…·ä½“
3. å¦‚æœæ˜¯ä¸“ä¸šé—®é¢˜ï¼Œä½¿ç”¨æ›´å‡†ç¡®çš„æœ¯è¯­
4. é•¿åº¦æ§åˆ¶åœ¨20-50å­—ä¹‹é—´

è¯·ç›´æ¥è¿”å›é‡å†™åçš„æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"""
        
        # æ”¶é›†é‡å†™ç»“æœ
        rewritten_query = original_query  # é»˜è®¤ä½¿ç”¨åŸæŸ¥è¯¢
        async for chunk in llm_service.stream_response(system_prompt, "è¯·é‡å†™æŸ¥è¯¢è¯­å¥"):
            if chunk.strip():
                rewritten_query = chunk.strip()
                break
        
        return {
            "messages": messages + [HumanMessage(content=rewritten_query)],
            "current_node": "rewrite",
            "node_history": state.get("node_history", []) + [{
                "node": "rewrite",
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "original_query": original_query[:100],
                "rewritten_query": rewritten_query[:100]
            }]
        }
    except Exception as e:
        print(f"âŒ é‡å†™èŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "error": f"é‡å†™å¤±è´¥: {str(e)}",
            "current_node": "rewrite",
            "node_history": state.get("node_history", []) + [{
                "node": "rewrite",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }

async def generate_node(state: AgentState, config: dict) -> dict:
    """ç”ŸæˆèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆå›ç­”"""
    try:
        documents = state.get("documents", [])
        messages = state.get("messages", [])
        
        if not documents:
            return {
                "generation": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚",
                "current_node": "generate",
                "node_history": state.get("node_history", []) + [{
                    "node": "generate",
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "note": "no_documents_found"
                }]
            }
        
        # è·å–ç”¨æˆ·æŸ¥è¯¢
        last_message = messages[-1]
        query = getattr(last_message, 'content', str(last_message))
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(documents[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªæ–‡æ¡£
            score = doc.get("score", 0)
            title = doc.get("title", "æ— æ ‡é¢˜")
            content = doc.get("content", "")
            context_parts.append(f"ã€æ–‡æ¡£{i+1} - {title} (ç›¸å…³åº¦: {score:.2%})ã€‘")
            context_parts.append(content[:800])  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£é•¿åº¦
            context_parts.append("---")
        
        context_text = "\n".join(context_parts)
        
        # ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        llm_service = get_llm_service()
        
        # æ”¶é›†ç”Ÿæˆç»“æœ
        full_response = ""
        async for chunk in llm_service.stream_response(query, context_text):
            full_response += chunk
        
        return {
            "generation": full_response,
            "current_node": "generate",
            "node_history": state.get("node_history", []) + [{
                "node": "generate",
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "documents_used": len(documents[:3]),
                "response_length": len(full_response)
            }]
        }
    except Exception as e:
        print(f"âŒ ç”ŸæˆèŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "error": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
            "current_node": "generate",
            "node_history": state.get("node_history", []) + [{
                "node": "generate",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }

async def fallback_node(state: AgentState, config: dict) -> dict:
    """å›é€€èŠ‚ç‚¹ - å¤„ç†æ— æ³•å›ç­”çš„æƒ…å†µ"""
    return {
        "generation": "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n2. å°†é—®é¢˜è¡¨è¿°å¾—æ›´å…·ä½“\n3. ä¸Šä¼ ç›¸å…³æ–‡æ¡£åˆ°çŸ¥è¯†åº“",
        "current_node": "fallback",
        "node_history": state.get("node_history", []) + [{
            "node": "fallback",
            "timestamp": datetime.now().isoformat(),
            "status": "success",
            "note": "fallback_response"
        }]
    }

async def decide_tools_node(state: AgentState, config: dict) -> dict:
    """å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·èŠ‚ç‚¹"""
    try:
        messages = state.get("messages", [])
        last_message = messages[-1]
        query = getattr(last_message, 'content', str(last_message))
        
        # ä½¿ç”¨ LLM åˆ¤æ–­æ˜¯å¦éœ€è¦å·¥å…·
        llm_service = get_llm_service()
        
        system_prompt = f"""åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å¤–éƒ¨å·¥å…·æ¥è§£ç­”ã€‚
        
ç”¨æˆ·é—®é¢˜: {query}

è¯·åˆ†æï¼š
1. æ˜¯å¦éœ€è¦å®æ—¶ä¿¡æ¯ï¼ˆå¤©æ°”ã€æ–°é—»ã€è‚¡ç¥¨ç­‰ï¼‰ï¼Ÿ
2. æ˜¯å¦éœ€è¦è®¡ç®—æˆ–å•ä½è½¬æ¢ï¼Ÿ
3. æ˜¯å¦éœ€è¦æœç´¢æœ€æ–°ç½‘ç»œä¿¡æ¯ï¼Ÿ
4. æ˜¯å¦éœ€è¦åœ¨çŸ¥è¯†åº“åŸºç¡€ä¸Šè¡¥å……å¤–éƒ¨ä¿¡æ¯ï¼Ÿ

å¦‚æœéœ€è¦ä»»ä½•å·¥å…·ï¼Œå›ç­”"yes"ï¼Œå¦åˆ™å›ç­”"no"ã€‚
åªå›ç­”"yes"æˆ–"no"ï¼Œä¸è¦è§£é‡Šã€‚"""
        
        response = ""
        async for chunk in llm_service.stream_response(system_prompt, "è¯·åˆ†ææ˜¯å¦éœ€è¦å·¥å…·"):
            response += chunk
        
        should_use_tools = "yes" in response.lower()
        
        return {
            "should_use_tools": should_use_tools,
            "current_node": "decide_tools",
            "node_history": state.get("node_history", []) + [{
                "node": "decide_tools",
                "timestamp": datetime.now().isoformat(),
                "status": "success",
                "decision": should_use_tools,
                "reason": response[:100]
            }]
        }
    except Exception as e:
        print(f"âŒ å·¥å…·å†³ç­–èŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "should_use_tools": False,
            "current_node": "decide_tools",
            "node_history": state.get("node_history", []) + [{
                "node": "decide_tools",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }

async def tools_node(state: AgentState, config: dict) -> dict:
    """å·¥å…·è°ƒç”¨èŠ‚ç‚¹"""
    try:
        tool_manager = get_tool_manager()
        tools = tool_manager.get_tools_list()
        
        # ç»‘å®šå·¥å…·åˆ°LLM
        llm_service = get_llm_service()
        llm_with_tools = llm_service.bind_tools(tools)
        
        # è·å–å¯¹è¯å†å²
        messages = state.get("messages", [])
        
        # è°ƒç”¨LLMï¼ˆå®ƒä¼šå†³å®šä½¿ç”¨å“ªä¸ªå·¥å…·ï¼‰
        response = await llm_with_tools.ainvoke(messages)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            tool_calls = []
            tool_results = []
            
            for tool_call in response.tool_calls:
                # æ‰§è¡Œå·¥å…·
                result = await tool_manager.execute_tool(
                    tool_call['name'], 
                    **tool_call['args']
                )
                
                tool_calls.append(tool_call)
                tool_results.append(result)
                
                # æ·»åŠ å·¥å…·æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                tool_message = ToolMessage(
                    content=json.dumps(result, ensure_ascii=False),
                    tool_call_id=tool_call.get('id', f"call_{len(tool_calls)}")
                )
                messages.append(tool_message)
            
            return {
                "messages": messages + [response],  # æ·»åŠ AIçš„å“åº”
                "tool_calls": tool_calls,
                "tool_results": tool_results,
                "current_node": "tools",
                "node_history": state.get("node_history", []) + [{
                    "node": "tools",
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "tools_called": len(tool_calls),
                    "tool_names": [tc['name'] for tc in tool_calls]
                }]
            }
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”
            return {
                "messages": messages + [response],
                "generation": response.content,
                "current_node": "tools",
                "node_history": state.get("node_history", []) + [{
                    "node": "tools",
                    "timestamp": datetime.now().isoformat(),
                    "status": "success",
                    "note": "no_tools_called"
                }]
            }
            
    except Exception as e:
        print(f"âŒ å·¥å…·èŠ‚ç‚¹å¤±è´¥: {e}")
        return {
            "error": f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}",
            "current_node": "tools",
            "node_history": state.get("node_history", []) + [{
                "node": "tools",
                "timestamp": datetime.now().isoformat(),
                "status": "error",
                "error": str(e)
            }]
        }


# ==================== è·¯ç”±é€»è¾‘ ====================

def should_retry(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•æ£€ç´¢"""
    retry_count = state.get("retry_count", 0)
    if retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
        return "rewrite"
    return "fallback"

def route_after_grade(state: AgentState) -> str:
    """è¯„ä¼°åçš„è·¯ç”±é€»è¾‘"""
    is_relevant = state.get("is_relevant")
    
    if is_relevant is None:
        return "generate"  # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤ç”Ÿæˆ
    
    if is_relevant:
        return "generate"
    else:
        # æ–‡æ¡£ä¸ç›¸å…³ï¼Œéœ€è¦é‡å†™æŸ¥è¯¢
        return "should_retry"

def route_after_retry(state: AgentState) -> str:
    """é‡è¯•åçš„è·¯ç”±é€»è¾‘"""
    retry_count = state.get("retry_count", 0)
    if retry_count >= 2:
        return "fallback"
    return "retrieve"

# ==================== æ„å»ºå·¥ä½œæµ ====================

def create_agent_workflow():
    """åˆ›å»ºä»£ç†å·¥ä½œæµ - å¢å¼ºç‰ˆï¼Œæ”¯æŒå·¥å…·è°ƒç”¨"""
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("start", start_node)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("grade", grade_node)
    workflow.add_node("rewrite", rewrite_node)
    workflow.add_node("generate", generate_node)
    workflow.add_node("fallback", fallback_node)
    workflow.add_node("decide_tools", decide_tools_node)  # æ–°å¢
    workflow.add_node("tools", tools_node)  # æ–°å¢
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("start")

    # å…ˆå†³å®šæ˜¯å¦ç”¨å·¥å…·
    workflow.add_edge("start", "decide_tools")

    # æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    workflow.add_conditional_edges(
        "decide_tools",
        lambda state: "tools" if state.get("should_use_tools") else "retrieve",
        {
            "tools": "tools",
            "retrieve": "retrieve"
        }
    )
    
    # å·¥å…·è°ƒç”¨åï¼Œè¿›å…¥æ­£å¸¸çš„RAGæµç¨‹æˆ–ç›´æ¥ç”Ÿæˆ
    workflow.add_conditional_edges(
        "tools",
        lambda state: "generate" if state.get("generation") else "retrieve",
        {
            "generate": "generate",
            "retrieve": "retrieve"
        }
    )
    
    # åŸæœ‰çš„RAGæµç¨‹ä¿æŒä¸å˜
    workflow.add_edge("retrieve", "grade")
    
    # æ¡ä»¶è¾¹ï¼šè¯„ä¼°åçš„è·¯ç”±
    workflow.add_conditional_edges(
        "grade",
        route_after_grade,
        {
            "generate": "generate",
            "should_retry": "should_retry"
        }
    )
    
    # æ¡ä»¶è¾¹ï¼šé‡è¯•å†³ç­–
    workflow.add_conditional_edges(
        "should_retry",
        should_retry,
        {
            "rewrite": "rewrite",
            "fallback": "fallback"
        }
    )
    
    workflow.add_edge("rewrite", "retrieve")
    workflow.add_edge("generate", END)
    workflow.add_edge("fallback", END)
    
    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile()
    
    return app

# å…¨å±€å·¥ä½œæµå®ä¾‹
_agent_workflow = None

def get_agent_workflow():
    """è·å–ä»£ç†å·¥ä½œæµå®ä¾‹"""
    global _agent_workflow
    if _agent_workflow is None:
        _agent_workflow = create_agent_workflow()
    return _agent_workflow

# ==================== è¾…åŠ©å‡½æ•° ====================

def format_workflow_debug(state: AgentState) -> dict:
    """æ ¼å¼åŒ–å·¥ä½œæµè°ƒè¯•ä¿¡æ¯"""
    return {
        "current_node": state.get("current_node", "unknown"),
        "node_history": state.get("node_history", []),
        "documents_count": len(state.get("documents", [])),
        "generation_length": len(state.get("generation", "")),
        "is_relevant": state.get("is_relevant"),
        "retry_count": state.get("retry_count", 0),
        "error": state.get("error"),
        "execution_time": str(datetime.now() - state.get("start_time", datetime.now()))
    }