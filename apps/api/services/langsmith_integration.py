"""
LangSmith é›†æˆæœåŠ¡ - æä¾›å…¨é“¾è·¯è¿½è¸ªã€æˆæœ¬ç›‘æ§å’Œæ€§èƒ½åˆ†æ
2026 ä¼ä¸šçº§ AI å¯è§‚æµ‹æ€§å¹³å°é›†æˆ
"""

import os
import json
import time
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from functools import wraps
import asyncio
import threading

# LangSmith æ ¸å¿ƒ
from langsmith import Client, traceable, RunTree
from langsmith.schemas import FeedbackCreate, Run

# å¯¼å…¥ç°æœ‰æœåŠ¡
from core.logger import logger
from models import User, Document, Entity, GraphEdge

# å…¨å±€ LangSmith å®¢æˆ·ç«¯
_langsmith_client = None

def get_langsmith_client() -> Client:
    """è·å– LangSmith å®¢æˆ·ç«¯å•ä¾‹"""
    global _langsmith_client
    if _langsmith_client is None:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        api_key = os.getenv("LANGCHAIN_API_KEY")
        endpoint = os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")
        
        if not api_key:
            logger.warning("âš ï¸ LangSmith API å¯†é’¥æœªé…ç½®ï¼Œå°†ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            _langsmith_client = None
        else:
            try:
                _langsmith_client = Client(
                    api_url=endpoint,
                    api_key=api_key,
                    timeout=30.0  # 30ç§’è¶…æ—¶
                )
                logger.info("âœ… LangSmith å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ LangSmith å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                _langsmith_client = None
    
    return _langsmith_client

class LangSmithMonitor:
    """LangSmith ç›‘æ§ç®¡ç†å™¨"""
    
    def __init__(self):
        self.client = get_langsmith_client()
        self.project_name = os.getenv("LANGCHAIN_PROJECT", "KnoSphere-Production-2026")
        self.environment = os.getenv("LANGCHAIN_ENVIRONMENT", "development")
        
        # æˆæœ¬è·Ÿè¸ª
        self.cost_tracker = CostTracker()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # è¯„ä¼°å™¨
        self.evaluator = AutoEvaluator()
        
        # ç”¨æˆ·åé¦ˆæ”¶é›†
        self.feedback_collector = FeedbackCollector()
    
    def is_enabled(self) -> bool:
        """æ£€æŸ¥ LangSmith æ˜¯å¦å¯ç”¨"""
        return self.client is not None and os.getenv("LANGCHAIN_TRACING_V2", "false").lower() == "true"
    
    def start_trace(self, 
                   name: str, 
                   inputs: Dict[str, Any],
                   run_type: str = "chain",
                   metadata: Optional[Dict[str, Any]] = None,
                   tags: Optional[List[str]] = None) -> Optional[RunTree]:
        """å¼€å§‹ä¸€ä¸ªæ–°çš„è¿½è¸ª"""
        if not self.is_enabled():
            return None
        
        try:
            run_tree = RunTree(
                name=name,
                run_type=run_type,
                inputs=inputs,
                project_name=self.project_name,
                metadata=metadata or {},
                tags=tags or [],
                extra={"environment": self.environment}
            )
            
            # å¯åŠ¨åå°çº¿ç¨‹å‘é€è¿½è¸ªï¼ˆé¿å…é˜»å¡ä¸»æµç¨‹ï¼‰
            threading.Thread(
                target=self._submit_trace,
                args=(run_tree,),
                daemon=True
            ).start()
            
            return run_tree
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨è¿½è¸ªå¤±è´¥: {e}")
            return None
    
    def _submit_trace(self, run_tree: RunTree):
        """æäº¤è¿½è¸ªåˆ° LangSmith"""
        try:
            self.client.create_run_tree(run_tree)
        except Exception as e:
            logger.error(f"âŒ æäº¤è¿½è¸ªå¤±è´¥: {e}")
    
    def end_trace(self, 
                  run_tree: Optional[RunTree], 
                  outputs: Dict[str, Any],
                  error: Optional[str] = None):
        """ç»“æŸè¿½è¸ª"""
        if not run_tree or not self.is_enabled():
            return
        
        try:
            run_tree.outputs = outputs
            run_tree.end_time = datetime.utcnow()
            
            if error:
                run_tree.error = error
            
            # æäº¤ç»“æŸçš„è¿½è¸ª
            self.client.update_run_tree(run_tree)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            if not error:
                duration = (run_tree.end_time - run_tree.start_time).total_seconds() * 1000
                self.performance_monitor.record_latency(run_tree.name, duration)
            
        except Exception as e:
            logger.error(f"âŒ ç»“æŸè¿½è¸ªå¤±è´¥: {e}")
    
    def log_feedback(self,
                    run_id: str,
                    score: float,
                    key: str = "user_feedback",
                    comment: Optional[str] = None,
                    source_info: Optional[Dict[str, Any]] = None):
        """è®°å½•ç”¨æˆ·åé¦ˆ"""
        if not self.is_enabled():
            return
        
        try:
            feedback = FeedbackCreate(
                key=key,
                score=score,
                comment=comment,
                run_id=run_id,
                source_info=source_info or {}
            )
            
            self.client.create_feedback(feedback)
            logger.info(f"âœ… ç”¨æˆ·åé¦ˆå·²è®°å½•: run_id={run_id}, score={score}")
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•åé¦ˆå¤±è´¥: {e}")
    
    def record_cost(self,
                   provider: str,
                   model: str,
                   input_tokens: int,
                   output_tokens: int,
                   user_id: Optional[int] = None):
        """è®°å½• Token æ¶ˆè€—æˆæœ¬"""
        self.cost_tracker.record_usage(
            provider=provider,
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            user_id=user_id
        )
    
    def get_performance_report(self, 
                             time_range_hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return self.performance_monitor.generate_report(time_range_hours)
    
    def get_cost_report(self,
                       start_date: Optional[datetime] = None,
                       end_date: Optional[datetime] = None) -> Dict[str, Any]:
        """è·å–æˆæœ¬æŠ¥å‘Š"""
        return self.cost_tracker.generate_report(start_date, end_date)
    
    def evaluate_response(self,
                         query: str,
                         context: str,
                         response: str,
                         run_id: Optional[str] = None) -> Dict[str, Any]:
        """è‡ªåŠ¨è¯„ä¼°å“åº”è´¨é‡"""
        return self.evaluator.evaluate(query, context, response, run_id)

class CostTracker:
    """Token æˆæœ¬è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        # æ¨¡å‹æˆæœ¬é…ç½®ï¼ˆç¾å…ƒ/1000 tokensï¼‰
        self.model_costs = {
            "deepseek-chat": {
                "input": float(os.getenv("DEEPSEEK_COST_PER_1K_INPUT", 0.00014)),
                "output": float(os.getenv("DEEPSEEK_COST_PER_1K_OUTPUT", 0.00028))
            },
            "qwen-max": {
                "input": float(os.getenv("ALIBABA_COST_PER_1K_INPUT", 0.0004)),
                "output": float(os.getenv("ALIBABA_COST_PER_1K_OUTPUT", 0.0008))
            },
            "gpt-4o-mini": {
                "input": 0.00015,
                "output": 0.0006
            },
            "gpt-3.5-turbo": {
                "input": 0.0005,
                "output": 0.0015
            }
        }
        
        # æˆæœ¬å­˜å‚¨ï¼ˆå†…å­˜ç¼“å­˜ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis æˆ–æ•°æ®åº“ï¼‰
        self.usage_records = []
    
    def record_usage(self,
                    provider: str,
                    model: str,
                    input_tokens: int,
                    output_tokens: int,
                    user_id: Optional[int] = None):
        """è®°å½• Token ä½¿ç”¨æƒ…å†µ"""
        try:
            # è®¡ç®—æˆæœ¬
            cost_config = self.model_costs.get(model, self.model_costs.get("deepseek-chat"))
            if not cost_config:
                logger.warning(f"æœªçŸ¥æ¨¡å‹æˆæœ¬é…ç½®: {model}")
                return
            
            input_cost = (input_tokens / 1000) * cost_config["input"]
            output_cost = (output_tokens / 1000) * cost_config["output"]
            total_cost = input_cost + output_cost
            
            # è®°å½•ä½¿ç”¨æƒ…å†µ
            record = {
                "timestamp": datetime.utcnow(),
                "provider": provider,
                "model": model,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "input_cost": input_cost,
                "output_cost": output_cost,
                "total_cost": total_cost,
                "user_id": user_id
            }
            
            self.usage_records.append(record)
            
            # è®°å½•æ—¥å¿—
            logger.info(f"ğŸ’° Token ä½¿ç”¨è®°å½•: {model}, "
                       f"è¾“å…¥: {input_tokens}, è¾“å‡º: {output_tokens}, "
                       f"æˆæœ¬: ${total_cost:.6f}")
            
            # å®šæœŸæ¸…ç†æ—§è®°å½•ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
            self._cleanup_old_records()
            
        except Exception as e:
            logger.error(f"âŒ è®°å½• Token ä½¿ç”¨å¤±è´¥: {e}")
    
    def _cleanup_old_records(self):
        """æ¸…ç†7å¤©å‰çš„è®°å½•"""
        cutoff_time = datetime.utcnow() - timedelta(days=7)
        self.usage_records = [
            r for r in self.usage_records 
            if r["timestamp"] > cutoff_time
        ]
    
    def generate_report(self,
                       start_date: Optional[datetime] = None,
                       end_date: Optional[datetime] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        try:
            # è®¾ç½®æ—¶é—´èŒƒå›´
            if not start_date:
                start_date = datetime.utcnow() - timedelta(days=7)
            if not end_date:
                end_date = datetime.utcnow()
            
            # ç­›é€‰è®°å½•
            filtered_records = [
                r for r in self.usage_records
                if start_date <= r["timestamp"] <= end_date
            ]
            
            # æŒ‰ç”¨æˆ·ç»Ÿè®¡
            user_stats = {}
            for record in filtered_records:
                user_id = record.get("user_id", "unknown")
                if user_id not in user_stats:
                    user_stats[user_id] = {
                        "total_tokens": 0,
                        "total_cost": 0.0,
                        "requests": 0
                    }
                
                user_stats[user_id]["total_tokens"] += record["input_tokens"] + record["output_tokens"]
                user_stats[user_id]["total_cost"] += record["total_cost"]
                user_stats[user_id]["requests"] += 1
            
            # æŒ‰æ¨¡å‹ç»Ÿè®¡
            model_stats = {}
            for record in filtered_records:
                model = record["model"]
                if model not in model_stats:
                    model_stats[model] = {
                        "total_tokens": 0,
                        "total_cost": 0.0,
                        "requests": 0
                    }
                
                model_stats[model]["total_tokens"] += record["input_tokens"] + record["output_tokens"]
                model_stats[model]["total_cost"] += record["total_cost"]
                model_stats[model]["requests"] += 1
            
            # è®¡ç®—æ€»è®¡
            total_tokens = sum(r["input_tokens"] + r["output_tokens"] for r in filtered_records)
            total_cost = sum(r["total_cost"] for r in filtered_records)
            total_requests = len(filtered_records)
            
            return {
                "time_range": {
                    "start": start_date.isoformat(),
                    "end": end_date.isoformat()
                },
                "summary": {
                    "total_requests": total_requests,
                    "total_tokens": total_tokens,
                    "total_cost": total_cost,
                    "avg_cost_per_request": total_cost / total_requests if total_requests > 0 else 0
                },
                "by_user": user_stats,
                "by_model": model_stats,
                "records_count": len(filtered_records)
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæˆæœ¬æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        # æ€§èƒ½æŒ‡æ ‡å­˜å‚¨
        self.metrics = {
            "latency": {},  # å„é˜¶æ®µå»¶è¿Ÿ
            "error_rate": {},  # å„é˜¶æ®µé”™è¯¯ç‡
            "token_usage": {},  # Token ä½¿ç”¨æƒ…å†µ
            "ttft": []  # é¦–å­—å»¶è¿Ÿ
        }
        
        # é˜ˆå€¼é…ç½®
        self.ttft_threshold = int(os.getenv("PERFORMANCE_TTFT_THRESHOLD_MS", 2000))
        self.token_limit = int(os.getenv("PERFORMANCE_TOKEN_LIMIT", 4000))
        self.error_rate_threshold = float(os.getenv("PERFORMANCE_ERROR_RATE_THRESHOLD", 0.05))
    
    def record_latency(self, operation: str, latency_ms: float):
        """è®°å½•æ“ä½œå»¶è¿Ÿ"""
        if operation not in self.metrics["latency"]:
            self.metrics["latency"][operation] = []
        
        self.metrics["latency"][operation].append({
            "timestamp": datetime.utcnow(),
            "latency_ms": latency_ms
        })
        
        # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        if len(self.metrics["latency"][operation]) > 1000:
            self.metrics["latency"][operation] = self.metrics["latency"][operation][-1000:]
    
    def record_ttft(self, ttft_ms: float):
        """è®°å½•é¦–å­—å»¶è¿Ÿ"""
        self.metrics["ttft"].append({
            "timestamp": datetime.utcnow(),
            "ttft_ms": ttft_ms
        })
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        if ttft_ms > self.ttft_threshold:
            logger.warning(f"âš ï¸ TTFT è¶…è¿‡é˜ˆå€¼: {ttft_ms}ms > {self.ttft_threshold}ms")
        
        # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        if len(self.metrics["ttft"]) > 1000:
            self.metrics["ttft"] = self.metrics["ttft"][-1000:]
    
    def record_error(self, operation: str, error_type: str):
        """è®°å½•é”™è¯¯"""
        if operation not in self.metrics["error_rate"]:
            self.metrics["error_rate"][operation] = {"total": 0, "errors": 0}
        
        self.metrics["error_rate"][operation]["total"] += 1
        self.metrics["error_rate"][operation]["errors"] += 1
    
    def record_success(self, operation: str):
        """è®°å½•æˆåŠŸ"""
        if operation not in self.metrics["error_rate"]:
            self.metrics["error_rate"][operation] = {"total": 0, "errors": 0}
        
        self.metrics["error_rate"][operation]["total"] += 1
    
    def check_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶å†µ"""
        health_status = {
            "status": "healthy",
            "checks": {},
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # æ£€æŸ¥ TTFT
        if self.metrics["ttft"]:
            avg_ttft = sum(r["ttft_ms"] for r in self.metrics["ttft"]) / len(self.metrics["ttft"])
            health_status["checks"]["ttft"] = {
                "status": "healthy" if avg_ttft <= self.ttft_threshold else "degraded",
                "value": avg_ttft,
                "threshold": self.ttft_threshold
            }
        
        # æ£€æŸ¥é”™è¯¯ç‡
        for operation, stats in self.metrics["error_rate"].items():
            if stats["total"] > 0:
                error_rate = stats["errors"] / stats["total"]
                health_status["checks"][f"error_rate_{operation}"] = {
                    "status": "healthy" if error_rate <= self.error_rate_threshold else "critical",
                    "value": error_rate,
                    "threshold": self.error_rate_threshold
                }
        
        # æ›´æ–°æ€»ä½“çŠ¶æ€
        if any(check["status"] == "critical" for check in health_status["checks"].values()):
            health_status["status"] = "critical"
        elif any(check["status"] == "degraded" for check in health_status["checks"].values()):
            health_status["status"] = "degraded"
        
        return health_status
    
    def generate_report(self, time_range_hours: int = 24) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            cutoff_time = datetime.utcnow() - timedelta(hours=time_range_hours)
            
            # ç­›é€‰ TTFT æ•°æ®
            ttft_records = [r for r in self.metrics["ttft"] if r["timestamp"] > cutoff_time]
            
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            report = {
                "time_range_hours": time_range_hours,
                "timestamp": datetime.utcnow().isoformat(),
                "ttft": {
                    "count": len(ttft_records),
                    "avg_ms": sum(r["ttft_ms"] for r in ttft_records) / len(ttft_records) if ttft_records else 0,
                    "p95_ms": self._calculate_percentile([r["ttft_ms"] for r in ttft_records], 95) if ttft_records else 0,
                    "p99_ms": self._calculate_percentile([r["ttft_ms"] for r in ttft_records], 99) if ttft_records else 0,
                    "max_ms": max(r["ttft_ms"] for r in ttft_records) if ttft_records else 0,
                    "threshold_exceeded": sum(1 for r in ttft_records if r["ttft_ms"] > self.ttft_threshold)
                },
                "latency_by_operation": {},
                "error_rates": {},
                "health": self.check_health()
            }
            
            # å„æ“ä½œå»¶è¿Ÿç»Ÿè®¡
            for operation, records in self.metrics["latency"].items():
                recent_records = [r for r in records if r["timestamp"] > cutoff_time]
                if recent_records:
                    latencies = [r["latency_ms"] for r in recent_records]
                    report["latency_by_operation"][operation] = {
                        "count": len(recent_records),
                        "avg_ms": sum(latencies) / len(latencies),
                        "p95_ms": self._calculate_percentile(latencies, 95),
                        "p99_ms": self._calculate_percentile(latencies, 99)
                    }
            
            # é”™è¯¯ç‡ç»Ÿè®¡
            for operation, stats in self.metrics["error_rate"].items():
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æœ‰æ—¶é—´ç­›é€‰
                if stats["total"] > 0:
                    report["error_rates"][operation] = {
                        "total_requests": stats["total"],
                        "errors": stats["errors"],
                        "error_rate": stats["errors"] / stats["total"]
                    }
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _calculate_percentile(self, values: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        index = (percentile / 100) * (len(sorted_values) - 1)
        
        if index.is_integer():
            return sorted_values[int(index)]
        else:
            lower = sorted_values[int(index)]
            upper = sorted_values[int(index) + 1]
            return lower + (upper - lower) * (index % 1)

class AutoEvaluator:
    """è‡ªåŠ¨è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.evaluation_model = os.getenv("EVALUATION_MODEL", "gpt-4o-mini")
        self.evaluation_provider = os.getenv("EVALUATION_MODEL_PROVIDER", "openai")
        
        # åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹
        self.eval_llm = self._init_evaluation_model()
    
    def _init_evaluation_model(self):
        """åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹"""
        try:
            if self.evaluation_provider == "openai":
                from langchain_openai import ChatOpenAI
                return ChatOpenAI(
                    model=self.evaluation_model,
                    temperature=0.0,  # è¯„ä¼°éœ€è¦ç¡®å®šæ€§
                    max_tokens=500
                )
            elif self.evaluation_provider == "alibaba":
                from langchain_community.chat_models import ChatTongyi
                return ChatTongyi(
                    model=self.evaluation_model,
                    temperature=0.0,
                    max_tokens=500
                )
            else:
                logger.warning(f"æœªçŸ¥è¯„ä¼°æ¨¡å‹æä¾›å•†: {self.evaluation_provider}")
                return None
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–è¯„ä¼°æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def evaluate(self,
                query: str,
                context: str,
                response: str,
                run_id: Optional[str] = None) -> Dict[str, Any]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        if not self.eval_llm:
            return {"error": "è¯„ä¼°æ¨¡å‹æœªåˆå§‹åŒ–"}
        
        try:
            # æ„å»ºè¯„ä¼°æç¤º
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI å“åº”è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚
è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¯„ä¼°å›ç­”è´¨é‡ï¼š

1. **ç›¸å…³æ€§ (Relevance)**: å›ç­”æ˜¯å¦ç›´æ¥ç›¸å…³äºç”¨æˆ·é—®é¢˜ (0-10åˆ†)
2. **å‡†ç¡®æ€§ (Accuracy)**: å›ç­”æ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæ˜¯å¦åŒ…å«å¹»è§‰ (0-10åˆ†)
3. **å®Œæ•´æ€§ (Completeness)**: å›ç­”æ˜¯å¦å®Œæ•´åœ°è§£å†³äº†ç”¨æˆ·é—®é¢˜ (0-10åˆ†)
4. **æ¸…æ™°åº¦ (Clarity)**: å›ç­”æ˜¯å¦æ¸…æ™°ã€æ˜“æ‡‚ (0-10åˆ†)
5. **å®‰å…¨æ€§ (Safety)**: å›ç­”æ˜¯å¦å®‰å…¨ã€æ— æœ‰å®³å†…å®¹ (0-10åˆ†)

è¯·è¾“å‡º JSON æ ¼å¼çš„è¯„ä¼°ç»“æœï¼š
{
  "scores": {
    "relevance": 0-10,
    "accuracy": 0-10,
    "completeness": 0-10,
    "clarity": 0-10,
    "safety": 0-10
  },
  "average_score": 0-10,
  "has_hallucination": true/false,
  "reason": "è¯„ä¼°ç†ç”±"
}"""

            evaluation_prompt = f"""
ç”¨æˆ·é—®é¢˜: {query}

æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡:
{context[:2000]}

AI å›ç­”:
{response[:2000]}

è¯·è¯„ä¼°å›ç­”è´¨é‡:
"""
            
            # è°ƒç”¨è¯„ä¼°æ¨¡å‹
            eval_response = self.eval_llm.invoke([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": evaluation_prompt}
            ])
            
            # è§£æå“åº”
            import json
            eval_result = json.loads(eval_response.content)
            
            # è®°å½•è¯„ä¼°ç»“æœåˆ° LangSmith
            if run_id:
                monitor = get_langsmith_monitor()
                if monitor.is_enabled():
                    monitor.log_feedback(
                        run_id=run_id,
                        score=eval_result["scores"]["accuracy"] / 10.0,  # å½’ä¸€åŒ–åˆ°0-1
                        key="auto_evaluation",
                        comment=f"è‡ªåŠ¨è¯„ä¼°: {eval_result['reason'][:200]}"
                    )
            
            return eval_result
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨è¯„ä¼°å¤±è´¥: {e}")
            return {"error": str(e)}

class FeedbackCollector:
    """ç”¨æˆ·åé¦ˆæ”¶é›†å™¨"""
    
    def __init__(self):
        self.client = get_langsmith_client()
    
    def collect_feedback(self,
                        feedback_data: Dict[str, Any]) -> bool:
        """æ”¶é›†ç”¨æˆ·åé¦ˆ"""
        try:
            run_id = feedback_data.get("run_id")
            if not run_id:
                logger.warning("ç¼ºå°‘ run_idï¼Œæ— æ³•å…³è”åé¦ˆ")
                return False
            
            # è§£æåé¦ˆæ•°æ®
            score = feedback_data.get("score", 0.5)
            comment = feedback_data.get("comment", "")
            feedback_type = feedback_data.get("type", "thumbs")
            user_id = feedback_data.get("user_id")
            
            # åˆ›å»ºåé¦ˆ
            feedback_key = f"user_{feedback_type}"
            
            if self.client:
                self.client.create_feedback(
                    run_id=run_id,
                    key=feedback_key,
                    score=score,
                    comment=comment,
                    source_info={"user_id": user_id} if user_id else {}
                )
            
            logger.info(f"âœ… ç”¨æˆ·åé¦ˆå·²æ”¶é›†: run_id={run_id}, score={score}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†ç”¨æˆ·åé¦ˆå¤±è´¥: {e}")
            return False
    
    def get_feedback_summary(self,
                           run_id: str) -> Dict[str, Any]:
        """è·å–åé¦ˆæ‘˜è¦"""
        try:
            if not self.client:
                return {"error": "LangSmith å®¢æˆ·ç«¯æœªåˆå§‹åŒ–"}
            
            feedbacks = self.client.list_feedback(run_ids=[run_id])
            
            summary = {
                "run_id": run_id,
                "total_feedback": len(feedbacks),
                "average_score": 0.0,
                "feedback_by_type": {}
            }
            
            if feedbacks:
                total_score = 0.0
                score_count = 0
                
                for feedback in feedbacks:
                    feedback_type = feedback.key
                    score = feedback.score
                    
                    if feedback_type not in summary["feedback_by_type"]:
                        summary["feedback_by_type"][feedback_type] = {
                            "count": 0,
                            "average_score": 0.0,
                            "comments": []
                        }
                    
                    summary["feedback_by_type"][feedback_type]["count"] += 1
                    
                    if score is not None:
                        total_score += score
                        score_count += 1
                        summary["feedback_by_type"][feedback_type]["average_score"] = (
                            (summary["feedback_by_type"][feedback_type]["average_score"] * 
                             (summary["feedback_by_type"][feedback_type]["count"] - 1) + score) /
                            summary["feedback_by_type"][feedback_type]["count"]
                        )
                    
                    if feedback.comment:
                        summary["feedback_by_type"][feedback_type]["comments"].append(feedback.comment)
                
                if score_count > 0:
                    summary["average_score"] = total_score / score_count
            
            return summary
            
        except Exception as e:
            logger.error(f"âŒ è·å–åé¦ˆæ‘˜è¦å¤±è´¥: {e}")
            return {"error": str(e)}

# è£…é¥°å™¨å‡½æ•°
def trace_function(name: str = None, run_type: str = "tool"):
    """è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            monitor = get_langsmith_monitor()
            
            # æ„å»ºè¾“å…¥
            inputs = {
                "args": str(args),
                "kwargs": kwargs
            }
            
            # å¼€å§‹è¿½è¸ª
            run_tree = monitor.start_trace(
                name=name or func.__name__,
                inputs=inputs,
                run_type=run_type,
                metadata={
                    "function": func.__name__,
                    "module": func.__module__
                }
            )
            
            try:
                # æ‰§è¡Œå‡½æ•°
                start_time = time.time()
                result = await func(*args, **kwargs)
                end_time = time.time()
                
                # è®°å½•æ€§èƒ½
                duration_ms = (end_time - start_time) * 1000
                monitor.performance_monitor.record_latency(func.__name__, duration_ms)
                
                # ç»“æŸè¿½è¸ª
                monitor.end_trace(
                    run_tree=run_tree,
                    outputs={"result": result},
                    error=None
                )
                
                return result
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                monitor.end_trace(
                    run_tree=run_tree,
                    outputs={},
                    error=str(e)
                )
                monitor.performance_monitor.record_error(func.__name__, type(e).__name__)
                raise e
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            monitor = get_langsmith_monitor()
            
            # æ„å»ºè¾“å…¥
            inputs = {
                "args": str(args),
                "kwargs": kwargs
            }
            
            # å¼€å§‹è¿½è¸ª
            run_tree = monitor.start_trace(
                name=name or func.__name__,
                inputs=inputs,
                run_type=run_type,
                metadata={
                    "function": func.__name__,
                    "module": func.__module__
                }
            )
            
            try:
                # æ‰§è¡Œå‡½æ•°
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                
                # è®°å½•æ€§èƒ½
                duration_ms = (end_time - start_time) * 1000
                monitor.performance_monitor.record_latency(func.__name__, duration_ms)
                
                # ç»“æŸè¿½è¸ª
                monitor.end_trace(
                    run_tree=run_tree,
                    outputs={"result": result},
                    error=None
                )
                
                return result
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                monitor.end_trace(
                    run_tree=run_tree,
                    outputs={},
                    error=str(e)
                )
                monitor.performance_monitor.record_error(func.__name__, type(e).__name__)
                raise e
        
        # æ ¹æ®å‡½æ•°ç±»å‹è¿”å›åŒ…è£…å™¨
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
_langsmith_monitor = None

def get_langsmith_monitor() -> LangSmithMonitor:
    """è·å– LangSmith ç›‘æ§å™¨å•ä¾‹"""
    global _langsmith_monitor
    if _langsmith_monitor is None:
        _langsmith_monitor = LangSmithMonitor()
    return _langsmith_monitor