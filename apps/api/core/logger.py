import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Optional
from loguru import logger

# ==================== å…¨å±€ logger å®ä¾‹ ====================
# åœ¨æ¨¡å—çº§åˆ«å¯¼å‡º loggerï¼Œè¿™æ ·å…¶ä»–æ¨¡å—å¯ä»¥ç›´æ¥å¯¼å…¥ä½¿ç”¨
# ä¾‹å¦‚ï¼šfrom core.logging import logger

# ==================== æ—¥å¿—é…ç½®å‡½æ•° ====================

def setup_logging():
    """é…ç½®ç»“æ„åŒ–æ—¥å¿—"""
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # ç§»é™¤é»˜è®¤é…ç½®
    logger.remove()
    
    # æ§åˆ¶å°è¾“å‡ºï¼šå¼€å‘ç¯å¢ƒçš„ç¾åŒ–ç‰ˆæœ¬
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
               "<level>{level: <8}</level> | "
               "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
               "<level>{message}</level>",
        level="INFO",
        colorize=True,
        backtrace=True,
        diagnose=True
    )
    
    # JSON æ–‡ä»¶è¾“å‡ºï¼šç”Ÿäº§ç¯å¢ƒçš„å¯è§£ææ ¼å¼
    logger.add(
        log_dir / "knosphere_api.json.log",
        format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} | {message}",
        level="DEBUG",
        rotation="10 MB",
        compression="zip",
        retention="30 days"
    )
    
    # è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼šåŒ…å«å·¥ä½œæµçŠ¶æ€
    logger.add(
        log_dir / "knosphere_workflow.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {extra[workflow]} | {message}",
        level="DEBUG",
        filter=lambda record: "workflow" in record["extra"],
        rotation="5 MB",
        retention="7 days"
    )
    
    # é”™è¯¯æ—¥å¿—å•ç‹¬å­˜å‚¨
    logger.add(
        log_dir / "knosphere_errors.log",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}\n{exception}",
        level="ERROR",
        rotation="1 MB",
        retention="90 days"
    )
    
    logger.info(f"âœ… æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œæ—¥å¿—ç›®å½•: {log_dir.absolute()}")
    return logger

# ==================== åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ ====================
# å¯é€‰ï¼šåœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨åˆå§‹åŒ–
# å¦‚æœä¸éœ€è¦è‡ªåŠ¨åˆå§‹åŒ–ï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
setup_logging()

# ==================== ç»“æ„åŒ–æ—¥å¿—å‡½æ•° ====================

class WorkflowLogger:
    """å·¥ä½œæµä¸“ç”¨æ—¥å¿—è®°å½•å™¨"""
    
    @staticmethod
    def node_start(node_name: str, state: Optional[dict] = None):
        """è®°å½•èŠ‚ç‚¹å¼€å§‹"""
        logger.bind(workflow=node_name).info(
            "ğŸš€ èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ",
            extra={
                "node": node_name,
                "state": state if state else {},
                "timestamp": datetime.now().isoformat(),
                "event": "node_start"
            }
        )
    
    @staticmethod
    def node_complete(node_name: str, result: dict, duration: float):
        """è®°å½•èŠ‚ç‚¹å®Œæˆ"""
        logger.bind(workflow=node_name).info(
            "âœ… èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ",
            extra={
                "node": node_name,
                "result": {k: v for k, v in result.items() if k != "documents"},
                "duration_seconds": round(duration, 3),
                "event": "node_complete"
            }
        )
    
    @staticmethod
    def node_error(node_name: str, error: Exception, state: Optional[dict] = None):
        """è®°å½•èŠ‚ç‚¹é”™è¯¯"""
        logger.bind(workflow=node_name).error(
            f"âŒ èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(error)}",
            extra={
                "node": node_name,
                "error": str(error),
                "error_type": type(error).__name__,
                "state": state if state else {},
                "event": "node_error"
            }
        )
    
    @staticmethod
    def workflow_start(query: str, workflow_id: Optional[str] = None):
        """è®°å½•å·¥ä½œæµå¼€å§‹"""
        workflow_id = workflow_id or f"wf_{datetime.now().timestamp()}"
        logger.bind(workflow="orchestrator").info(
            "ğŸš€ å·¥ä½œæµå¼€å§‹æ‰§è¡Œ",
            extra={
                "workflow_id": workflow_id,
                "query": query[:200],
                "timestamp": datetime.now().isoformat(),
                "event": "workflow_start"
            }
        )
        return workflow_id
    
    @staticmethod
    def workflow_complete(workflow_id: str, final_state: dict, total_duration: float):
        """è®°å½•å·¥ä½œæµå®Œæˆ"""
        logger.bind(workflow="orchestrator").info(
            "ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆ",
            extra={
                "workflow_id": workflow_id,
                "final_node": final_state.get("current_node"),
                "total_duration_seconds": round(total_duration, 3),
                "documents_processed": len(final_state.get("documents", [])),
                "generation_length": len(final_state.get("generation", "")),
                "retry_count": final_state.get("retry_count", 0),
                "event": "workflow_complete"
            }
        )

    @staticmethod
    def workflow_error(workflow_id: str, error: str, total_duration: float = 0):
        """è®°å½•å·¥ä½œæµé”™è¯¯"""
        logger.bind(workflow="orchestrator").error(
            f"ğŸ’¥ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error}",
            extra={
                "workflow_id": workflow_id,
                "error": error,
                "total_duration_seconds": round(total_duration, 3),
                "event": "workflow_error"
            }
        )
    
    
    @staticmethod
    def retrieval_log(query: str, documents: list, strategy: Optional[str] = None):
        """è®°å½•æ£€ç´¢æ—¥å¿—"""
        logger.bind(workflow="retrieval").debug(
            "ğŸ” æ–‡æ¡£æ£€ç´¢å®Œæˆ",
            extra={
                "query": query[:200],
                "documents_count": len(documents),
                "strategy": strategy,
                "top_documents": [
                    {
                        "title": doc.get("title", "æ— æ ‡é¢˜")[:50],
                        "score": doc.get("score", 0),
                        "content_preview": doc.get("content", "")[:100]
                    }
                    for doc in documents[:3]
                ] if documents else [],
                "event": "retrieval_complete"
            }
        )
    
    @staticmethod
    def generation_log(query: str, context_size: int, response_length: int):
        """è®°å½•ç”Ÿæˆæ—¥å¿—"""
        logger.bind(workflow="generation").debug(
            "ğŸ¤– AI ç”Ÿæˆå®Œæˆ",
            extra={
                "query": query[:200],
                "context_size_chars": context_size,
                "response_length_chars": response_length,
                "token_estimate": int(response_length / 4),  # ç²—ç•¥ä¼°ç®—
                "event": "generation_complete"
            }
        )

# ==================== API è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶ ====================

def log_api_request(request_data: dict, endpoint: str, user_agent: Optional[str] = None):
    """è®°å½• API è¯·æ±‚"""
    logger.info(
        "ğŸ“¥ API è¯·æ±‚æ¥æ”¶",
        extra={
            "endpoint": endpoint,
            "method": "POST",
            "user_agent": user_agent,
            "request_data": {
                "query": request_data.get("query", "")[:100],
                "top_k": request_data.get("top_k"),
                "final_k": request_data.get("final_k")
            },
            "timestamp": datetime.now().isoformat(),
            "event": "api_request"
        }
    )

def log_api_response(endpoint: str, status_code: int, response_time: float, error: Optional[str] = None):
    """è®°å½• API å“åº”"""
    if error:
        logger.error(
            "ğŸ“¤ API å“åº”é”™è¯¯",
            extra={
                "endpoint": endpoint,
                "status_code": status_code,
                "response_time_seconds": round(response_time, 3),
                "error": error,
                "event": "api_response_error"
            }
        )
    else:
        logger.info(
            "ğŸ“¤ API å“åº”æˆåŠŸ",
            extra={
                "endpoint": endpoint,
                "status_code": status_code,
                "response_time_seconds": round(response_time, 3),
                "event": "api_response_success"
            }
        )

# ==================== æ€§èƒ½ç›‘æ§ ====================

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            "retrieval_times": [],
            "generation_times": [],
            "workflow_times": [],
            "error_counts": {}
        }
    
    def record_metric(self, metric_type: str, value: float, **kwargs):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        if metric_type in self.metrics:
            if isinstance(self.metrics[metric_type], list):
                self.metrics[metric_type].append(value)
                # ä¿æŒæœ€è¿‘1000ä¸ªè®°å½•
                if len(self.metrics[metric_type]) > 1000:
                    self.metrics[metric_type] = self.metrics[metric_type][-1000:]
        
        # è®°å½•åˆ°æ—¥å¿—
        logger.debug(
            f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡: {metric_type} = {value:.3f}s",
            extra={
                "metric_type": metric_type,
                "value": value,
                "unit": "seconds",
                **kwargs,
                "event": "performance_metric"
            }
        )
    
    def get_summary(self) -> dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        for metric_type, values in self.metrics.items():
            if values and isinstance(values, list):
                summary[metric_type] = {
                    "count": len(values),
                    "avg": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values),
                    "p95": sorted(values)[int(len(values) * 0.95)] if len(values) > 1 else values[0]
                }
        
        return summary

# å…¨å±€æ€§èƒ½ç›‘æ§å™¨
_performance_monitor = None

def get_performance_monitor() -> PerformanceMonitor:
    """è·å–æ€§èƒ½ç›‘æ§å™¨å®ä¾‹"""
    global _performance_monitor
    if _performance_monitor is None:
        _performance_monitor = PerformanceMonitor()
    return _performance_monitor

# ==================== å¥åº·æ£€æŸ¥æ—¥å¿— ====================

def log_health_check():
    """è®°å½•å¥åº·æ£€æŸ¥"""
    import psutil
    import platform
    
    system_info = {
        "platform": platform.platform(),
        "python_version": platform.python_version(),
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_usage": psutil.disk_usage("/").percent,
        "process_memory_mb": psutil.Process().memory_info().rss / 1024 / 1024
    }
    
    logger.info(
        "ğŸ¥ ç³»ç»Ÿå¥åº·æ£€æŸ¥",
        extra={
            "system_info": system_info,
            "timestamp": datetime.now().isoformat(),
            "event": "health_check"
        }
    )
    
    return system_info

# ==================== å¯¼å‡ºé¡¹ ====================
# æ˜ç¡®å¯¼å‡ºå“ªäº›å†…å®¹å¯ä»¥è¢«å…¶ä»–æ¨¡å—å¯¼å…¥
__all__ = [
    'logger',           # loguru logger å®ä¾‹
    'setup_logging',    # æ—¥å¿—é…ç½®å‡½æ•°
    'WorkflowLogger',   # å·¥ä½œæµæ—¥å¿—ç±»
    'log_api_request',  # API è¯·æ±‚æ—¥å¿—å‡½æ•°
    'log_api_response', # API å“åº”æ—¥å¿—å‡½æ•°
    'PerformanceMonitor', # æ€§èƒ½ç›‘æ§ç±»
    'get_performance_monitor', # è·å–æ€§èƒ½ç›‘æ§å™¨
    'log_health_check', # å¥åº·æ£€æŸ¥å‡½æ•°
]