import os
import time
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Optional
from celery import Task
from sqlmodel import Session, select
from tasks.celery_app import celery_app
from core.logger import logger
from services.embedding import generate_vector
from services.graph_extractor import get_graph_extractor
from database import engine
from models import Document, Entity, GraphEdge

class BaseTaskWithDB(Task):
    """å¸¦æœ‰æ•°æ®åº“è¿æ¥çš„åŸºç¡€ä»»åŠ¡ç±»"""
    
    def __init__(self):
        super().__init__()
        self.db_session = None
    
    def before_start(self, task_id, args, kwargs):
        """ä»»åŠ¡å¼€å§‹å‰åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info(f"ä»»åŠ¡ {task_id} å¼€å§‹æ‰§è¡Œ")
        self.db_session = Session(engine)
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥æ—¶çš„å¤„ç†"""
        logger.error(f"ä»»åŠ¡ {task_id} å¤±è´¥: {exc}", exc_info=True)
        if self.db_session:
            self.db_session.rollback()
    
    def after_return(self, status, retval, task_id, args, kwargs, einfo):
        """ä»»åŠ¡è¿”å›åçš„æ¸…ç†"""
        if self.db_session:
            self.db_session.close()
        logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆï¼ŒçŠ¶æ€: {status}")

# ==================== æ–‡ä»¶å¤„ç†è¾…åŠ©å‡½æ•° ====================

def _process_pdf(file_path: str) -> tuple[str, int]:
    """å¤„ç† PDF æ–‡ä»¶"""
    try:
        import PyPDF2
        
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            
            content = ""
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                text = page.extract_text()
                if text:  # ç¡®ä¿æœ‰å†…å®¹
                    content += text + "\n\n"
                
                # æ¯å¤„ç†10é¡µè®°å½•ä¸€æ¬¡è¿›åº¦
                if page_num % 10 == 0 and page_num > 0:
                    logger.debug(f"ğŸ“„ å·²å¤„ç† {page_num+1}/{total_pages} é¡µ")
            
            logger.info(f"âœ… PDF è§£æå®Œæˆ: {total_pages} é¡µ")
            return content, total_pages
            
    except ImportError:
        raise ImportError("è¯·å®‰è£… PyPDF2: pip install PyPDF2")
    except Exception as e:
        raise Exception(f"PDF å¤„ç†å¤±è´¥: {e}")

def _process_docx(file_path: str) -> tuple[str, int]:
    """å¤„ç† DOCX æ–‡ä»¶"""
    try:
        import docx
        
        doc = docx.Document(file_path)
        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
        
        content = "\n".join(paragraphs)
        logger.info(f"âœ… DOCX è§£æå®Œæˆ: {len(paragraphs)} æ®µè½")
        
        return content, len(paragraphs)
        
    except ImportError:
        raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")
    except Exception as e:
        raise Exception(f"DOCX å¤„ç†å¤±è´¥: {e}")

def _process_text(file_path: str) -> tuple[str, int]:
    """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
    try:
        # å°è¯•å¤šç§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding, errors='ignore') as file:
                    content = file.read()
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                if content and len(content) > 10:
                    lines = content.count('\n') + 1
                    logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶è§£æå®Œæˆ ({encoding}ç¼–ç ): {lines} è¡Œ")
                    return content, lines
                    
            except UnicodeDecodeError:
                continue
            except Exception:
                continue
        
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨äºŒè¿›åˆ¶è¯»å–
        with open(file_path, 'rb') as file:
            content = file.read().decode('utf-8', errors='ignore')
        
        lines = content.count('\n') + 1
        logger.warning(f"âš ï¸ ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼è§£ææ–‡æœ¬æ–‡ä»¶: {lines} è¡Œ")
        
        return content, lines
        
    except Exception as e:
        raise Exception(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

def _split_into_chunks(text: str, max_chunk_size: int = 2000, overlap: int = 200) -> list[str]:
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºé‡å çš„å—"""
    if not text:
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = min(start + max_chunk_size, text_length)
        
        # å¦‚æœä¸åœ¨æ®µè½è¾¹ç•Œï¼Œå‘å‰æ‰¾åˆé€‚çš„è¾¹ç•Œ
        if end < text_length:
            # å°è¯•åœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
            paragraph_end = text.find('\n\n', start + int(max_chunk_size * 0.8))
            if paragraph_end != -1 and paragraph_end < end + 500:
                end = paragraph_end + 2  # åŒ…æ‹¬æ¢è¡Œç¬¦
            else:
                # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
                sentence_end = text.find('. ', start + int(max_chunk_size * 0.8))
                if sentence_end != -1 and sentence_end < end + 300:
                    end = sentence_end + 1
        
        chunk = text[start:end]
        chunks.append(chunk)
        
        # é‡å æ»‘åŠ¨
        start = max(start + 1, end - overlap)  # é˜²æ­¢ start ä¸å¢åŠ 
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if start >= text_length - 100 or start >= text_length:
            # æ·»åŠ æœ€åä¸€å—
            if start < text_length:
                chunks.append(text[start:text_length])
            break
    
    return chunks

# ==================== ä¸»ä»»åŠ¡å‡½æ•° ====================

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.process_large_document")
def process_large_document(self, file_path: str, doc_id: int, user_id: Optional[str] = None):
    """
    å¤„ç†å¤§æ–‡æ¡£ä»»åŠ¡ï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±æå–ï¼‰
    
    å‚æ•°:
    - file_path: æ–‡ä»¶è·¯å¾„
    - doc_id: æ–‡æ¡£ID
    - user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ å¼€å§‹åå°å¤„ç†æ–‡æ¡£ ID: {doc_id}, ä»»åŠ¡ID: {task_id}")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # 2. æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
        file_ext = os.path.splitext(file_path)[1].lower()
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path} ({file_size:.2f}MB)")
        
        # 3. è¯»å–æ–‡ä»¶å†…å®¹
        content = ""
        total_steps = 0
        
        if file_ext == '.pdf':
            content, total_steps = _process_pdf(file_path)
        elif file_ext == '.docx':
            content, total_steps = _process_docx(file_path)
        elif file_ext in ['.txt', '.md']:
            content, total_steps = _process_text(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        if not content or len(content) < 50:
            raise ValueError(f"æ–‡æ¡£å†…å®¹å¤ªçŸ­æˆ–æ— æ•ˆ: {len(content)} å­—ç¬¦")
        
        # 4. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ20%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 1,
                'total': 5,
                'stage': 'æ–‡ä»¶è§£æå®Œæˆ',
                'progress': 20,
                'details': f"å·²è§£æ {len(content)} å­—ç¬¦"
            }
        )
        
        # 5. åˆ†å‰²æ–‡æ¡£ä¸ºå—
        chunks = _split_into_chunks(content, max_chunk_size=2000)
        logger.info(f"ğŸ“‘ æ–‡æ¡£åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        
        # 6. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ40%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 2,
                'total': 5,
                'stage': 'æ–‡æ¡£åˆ†å‰²å®Œæˆ',
                'progress': 40,
                'details': f"å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—"
            }
        )
        
        # 7. ä¸ºæ¯ä¸ªå—ç”Ÿæˆå‘é‡
        vectors = []
        
        for i, chunk in enumerate(chunks):
            try:
                # æ ¹æ®å—çš„å¤§å°åŠ¨æ€é€‰æ‹©ç»´åº¦
                chunk_len = len(chunk)
                if chunk_len > 1000:
                    mode = "precise"
                elif chunk_len > 500:
                    mode = "balanced"
                else:
                    mode = "fast"
                
                # ç”Ÿæˆå‘é‡
                vector = asyncio.run(generate_vector(chunk, mode=mode))
                vectors.append(vector)
                
                # æ›´æ–°å­è¿›åº¦
                if i % max(1, len(chunks) // 10) == 0:
                    progress = 40 + (i / len(chunks)) * 20
                    self.update_state(
                        state='PROGRESS',
                        meta={
                            'current': 2 + (i / len(chunks)),
                            'total': 5,
                            'stage': 'å‘é‡ç”Ÿæˆä¸­',
                            'progress': progress,
                            'details': f"å·²ç”Ÿæˆ {i+1}/{len(chunks)} ä¸ªå‘é‡"
                        }
                    )
                    
            except Exception as e:
                logger.warning(f"å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œå— {i+1}: {e}")
                # ä½¿ç”¨ç©ºå‘é‡ä½œä¸ºå ä½ç¬¦
                vectors.append([0.0] * 1536)  # å‡è®¾1536ç»´
        
        # 8. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ60%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 3,
                'total': 5,
                'stage': 'å‘é‡ç”Ÿæˆå®Œæˆ',
                'progress': 60,
                'details': f"å·²ç”Ÿæˆ {len(vectors)} ä¸ªå‘é‡"
            }
        )
        
        # 9. å­˜å‚¨åˆ°æ•°æ®åº“
        file_name = os.path.basename(file_path)
        with self.db_session as session:
            # æ›´æ–°æˆ–åˆ›å»ºæ–‡æ¡£è®°å½•
            document = session.get(Document, doc_id)
            if not document:
                document = Document(
                    id=doc_id,
                    title=file_name,
                    content=content[:10000] + "..." if len(content) > 10000 else content,
                    embedding=vectors[0] if vectors else None,
                    user_id=user_id,
                    graph_extracted=False
                )
                session.add(document)
            else:
                document.title = file_name
                document.content = content[:10000] + "..." if len(content) > 10000 else content
                document.embedding = vectors[0] if vectors else None
                document.graph_extracted = False
            
            session.commit()
            session.refresh(document)
            
            # è®°å½•å‘é‡åŒ–ç»“æœ
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} å‘é‡åŒ–å®Œæˆï¼Œå­˜å‚¨ {len(vectors)} ä¸ªå‘é‡")
            
            # 10. æå–çŸ¥è¯†å›¾è°±
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': 4,
                    'total': 5,
                    'stage': 'å¼€å§‹çŸ¥è¯†å›¾è°±æå–',
                    'progress': 70,
                    'details': "æ­£åœ¨ä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»..."
                }
            )
            
            try:
                # åˆ›å»ºå›¾è°±æå–å™¨
                extractor = get_graph_extractor(session)
                
                # æå–å›¾è°±ï¼ˆåŒæ­¥è°ƒç”¨å¼‚æ­¥å‡½æ•°ï¼‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    graph_result = loop.run_until_complete(
                        extractor.extract_from_document(document)
                    )
                    
                    if graph_result['success']:
                        logger.info(f"âœ… æ–‡æ¡£ {doc_id} çŸ¥è¯†å›¾è°±æå–æˆåŠŸ: "
                                   f"{graph_result['entities_saved']} å®ä½“, "
                                   f"{graph_result['relations_saved']} å…³ç³»")
                        
                        self.update_state(
                            state='PROGRESS',
                            meta={
                                'current': 4.5,
                                'total': 5,
                                'stage': 'çŸ¥è¯†å›¾è°±æå–å®Œæˆ',
                                'progress': 85,
                                'details': f"æå–äº† {graph_result['entities_saved']} ä¸ªå®ä½“å’Œ {graph_result['relations_saved']} ä¸ªå…³ç³»"
                            }
                        )
                    else:
                        logger.warning(f"âš ï¸ æ–‡æ¡£ {doc_id} å›¾è°±æå–å¤±è´¥: {graph_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                finally:
                    loop.close()
                    
            except Exception as e:
                logger.error(f"âŒ çŸ¥è¯†å›¾è°±æå–å¤±è´¥: {e}", exc_info=True)
                # å›¾è°±æå–å¤±è´¥ä¸åº”ä¸­æ–­æ•´ä¸ªä»»åŠ¡ï¼Œåªè®°å½•æ—¥å¿—
        
            # 11. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.debug(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            # 12. æœ€ç»ˆè¿›åº¦æ›´æ–°
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': 5,
                    'total': 5,
                    'stage': 'å¤„ç†å®Œæˆ',
                    'progress': 100,
                    'details': f"æ–‡æ¡£å·²æˆåŠŸå¤„ç†ï¼ŒID: {doc_id}"
                }
            )
            
            return {
                "status": "completed",
                "doc_id": doc_id,
                "title": file_name,
                "chunks_count": len(chunks),
                "vectors_count": len(vectors),
                "content_length": len(content),
                "graph_extracted": document.graph_extracted,
                "task_id": task_id
            }
    
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}", exc_info=True)
        raise self.retry(exc=e, countdown=60)

# ==================== çŸ¥è¯†å›¾è°±æå–ä»»åŠ¡ ====================

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.extract_graph_from_document")
def extract_graph_from_document(self, document_id: int, user_id: Optional[str] = None):
    """ç‹¬ç«‹çš„çŸ¥è¯†å›¾è°±æå–ä»»åŠ¡"""
    task_id = self.request.id
    logger.info(f"ğŸ§  å¼€å§‹çŸ¥è¯†å›¾è°±æå–ï¼Œæ–‡æ¡£ID: {document_id}, ä»»åŠ¡ID: {task_id}")
    
    try:
        with self.db_session as session:
            # è·å–æ–‡æ¡£
            document = session.get(Document, document_id)
            
            if not document:
                self.update_state(state='FAILURE', meta={'error': 'æ–‡æ¡£ä¸å­˜åœ¨'})
                return {"error": "æ–‡æ¡£ä¸å­˜åœ¨"}
            
            # å¦‚æœå·²ç»æå–è¿‡ï¼Œè·³è¿‡
            if document.graph_extracted:
                self.update_state(
                    state='SUCCESS',
                    meta={'message': 'å·²æå–è¿‡', 'doc_id': document_id}
                )
                return {"message": "å·²æå–è¿‡", "doc_id": document_id}
            
            # æ£€æŸ¥æ–‡æ¡£å†…å®¹
            if not document.content or len(document.content) < 50:
                self.update_state(state='FAILURE', meta={'error': 'æ–‡æ¡£å†…å®¹å¤ªçŸ­'})
                return {"error": "æ–‡æ¡£å†…å®¹å¤ªçŸ­"}
            
            # åˆ›å»ºæå–å™¨
            extractor = get_graph_extractor(session)
            
            # æ›´æ–°ä»»åŠ¡è¿›åº¦
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': 1,
                    'total': 4,
                    'stage': 'å¼€å§‹æå–',
                    'progress': 25,
                    'details': 'æ­£åœ¨åˆå§‹åŒ–å›¾è°±æå–å™¨...'
                }
            )
            
            # æå–å›¾è°±
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # æå–å®ä½“
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': 2,
                        'total': 4,
                        'stage': 'æå–å®ä½“',
                        'progress': 50,
                        'details': 'æ­£åœ¨ä»æ–‡æ¡£ä¸­æå–å®ä½“...'
                    }
                )
                
                graph_result = loop.run_until_complete(
                    extractor.extract_from_document(document)
                )
                
                if graph_result['success']:
                    # æ›´æ–°æ–‡æ¡£çŠ¶æ€
                    document.graph_extracted = True
                    document.graph_extraction_time = time.time()
                    session.commit()
                    
                    self.update_state(
                        state='SUCCESS',
                        meta={
                            'message': 'å›¾è°±æå–æˆåŠŸ',
                            'doc_id': document_id,
                            'entities': graph_result['entities_saved'],
                            'relations': graph_result['relations_saved']
                        }
                    )
                    
                    logger.info(f"âœ… çŸ¥è¯†å›¾è°±æå–æˆåŠŸ: æ–‡æ¡£ {document_id}, "
                               f"å®ä½“: {graph_result['entities_saved']}, "
                               f"å…³ç³»: {graph_result['relations_saved']}")
                    
                    return {
                        "success": True,
                        "doc_id": document_id,
                        "entities": graph_result['entities_saved'],
                        "relations": graph_result['relations_saved'],
                        "entities_list": [e.get('name', '') for e in graph_result.get('entities', [])][:10]
                    }
                else:
                    self.update_state(
                        state='FAILURE',
                        meta={'error': graph_result.get('error', 'æœªçŸ¥é”™è¯¯')}
                    )
                    return {
                        "success": False,
                        "error": graph_result.get('error', 'æœªçŸ¥é”™è¯¯')
                    }
                    
            except Exception as e:
                logger.error(f"å›¾è°±æå–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
                self.update_state(state='FAILURE', meta={'error': str(e)})
                return {"success": False, "error": str(e)}
            finally:
                loop.close()
                
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æå–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        self.update_state(state='FAILURE', meta={'error': str(e)})
        return {"success": False, "error": str(e)}

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.batch_extract_graphs")
def batch_extract_graphs(self, document_ids: List[int], user_id: Optional[str] = None):
    """æ‰¹é‡æå–çŸ¥è¯†å›¾è°±"""
    task_id = self.request.id
    logger.info(f"ğŸ§  å¼€å§‹æ‰¹é‡æå–çŸ¥è¯†å›¾è°±ï¼Œå…± {len(document_ids)} ä¸ªæ–‡æ¡£ï¼Œä»»åŠ¡ID: {task_id}")
    
    results = []
    with self.db_session as session:
        for i, doc_id in enumerate(document_ids):
            try:
                # è·å–æ–‡æ¡£
                document = session.get(Document, doc_id)
                if not document:
                    results.append({
                        "doc_id": doc_id,
                        "status": "failed",
                        "error": "æ–‡æ¡£ä¸å­˜åœ¨"
                    })
                    continue
                
                # å¦‚æœå·²ç»æå–è¿‡ï¼Œè·³è¿‡
                if document.graph_extracted:
                    results.append({
                        "doc_id": doc_id,
                        "status": "skipped",
                        "message": "å·²æå–è¿‡"
                    })
                    continue
                
                # æ£€æŸ¥æ–‡æ¡£å†…å®¹
                if not document.content or len(document.content) < 50:
                    results.append({
                        "doc_id": doc_id,
                        "status": "skipped",
                        "reason": "å†…å®¹å¤ªçŸ­"
                    })
                    continue
                
                # åˆ›å»ºæå–å™¨
                extractor = get_graph_extractor(session)
                
                # æå–å›¾è°±
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    graph_result = loop.run_until_complete(
                        extractor.extract_from_document(document)
                    )
                    
                    if graph_result['success']:
                        document.graph_extracted = True
                        document.graph_extraction_time = time.time()
                        session.commit()
                        
                        results.append({
                            "doc_id": doc_id,
                            "status": "success",
                            "entities": graph_result['entities_saved'],
                            "relations": graph_result['relations_saved']
                        })
                    else:
                        results.append({
                            "doc_id": doc_id,
                            "status": "failed",
                            "error": graph_result.get('error', 'æœªçŸ¥é”™è¯¯')
                        })
                finally:
                    loop.close()
                
            except Exception as e:
                logger.error(f"æ–‡æ¡£ {doc_id} å›¾è°±æå–å¤±è´¥: {e}")
                results.append({
                    "doc_id": doc_id,
                    "status": "failed",
                    "error": str(e)
                })
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(document_ids) * 100
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': i + 1,
                    'total': len(document_ids),
                    'stage': 'æå–ä¸­',
                    'progress': progress,
                    'details': f"å·²å¤„ç† {i+1}/{len(document_ids)} ä¸ªæ–‡æ¡£"
                }
            )
    
    success_count = len([r for r in results if r['status'] == 'success'])
    failed_count = len([r for r in results if r['status'] == 'failed'])
    skipped_count = len([r for r in results if r['status'] in ['skipped', 'skipped']])
    
    return {
        "status": "completed",
        "total_docs": len(document_ids),
        "success_count": success_count,
        "failed_count": failed_count,
        "skipped_count": skipped_count,
        "results": results,
        "task_id": task_id
    }

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.reprocess_all_graphs")
def reprocess_all_graphs(self, user_id: Optional[str] = None):
    """é‡æ–°æå–æ‰€æœ‰æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±"""
    task_id = self.request.id
    logger.info(f"ğŸ”„ å¼€å§‹é‡æ–°æå–æ‰€æœ‰æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ï¼Œä»»åŠ¡ID: {task_id}")
    
    with self.db_session as session:
        # è·å–æ‰€æœ‰æ–‡æ¡£
        if user_id:
            documents = session.exec(
                select(Document).where(Document.user_id == user_id)
            ).all()
        else:
            documents = session.exec(select(Document)).all()
        
        logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        # é‡ç½®æ‰€æœ‰æ–‡æ¡£çš„å›¾è°±æå–çŠ¶æ€
        for doc in documents:
            doc.graph_extracted = False
            doc.graph_extraction_time = None
        
        session.commit()
        
        # æå–å›¾è°±
        extractor = get_graph_extractor(session)
        
        results = []
        for i, document in enumerate(documents):
            try:
                # è·³è¿‡å†…å®¹å¤ªçŸ­çš„æ–‡æ¡£
                if not document.content or len(document.content) < 50:
                    results.append({
                        "doc_id": document.id,
                        "title": document.title,
                        "status": "skipped",
                        "reason": "å†…å®¹å¤ªçŸ­"
                    })
                    continue
                
                # æå–å›¾è°±
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    graph_result = loop.run_until_complete(
                        extractor.extract_from_document(document)
                    )
                    
                    if graph_result['success']:
                        document.graph_extracted = True
                        document.graph_extraction_time = time.time()
                        
                        results.append({
                            "doc_id": document.id,
                            "title": document.title,
                            "status": "success",
                            "entities": graph_result['entities_saved'],
                            "relations": graph_result['relations_saved']
                        })
                    else:
                        results.append({
                            "doc_id": document.id,
                            "title": document.title,
                            "status": "failed",
                            "error": graph_result.get('error', 'æœªçŸ¥é”™è¯¯')
                        })
                finally:
                    loop.close()
                
            except Exception as e:
                logger.error(f"æ–‡æ¡£ {document.id} å›¾è°±æå–å¤±è´¥: {e}")
                results.append({
                    "doc_id": document.id,
                    "title": document.title,
                    "status": "failed",
                    "error": str(e)
                })
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(documents) * 100
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': i + 1,
                    'total': len(documents),
                    'stage': 'é‡æ–°æå–ä¸­',
                    'progress': progress,
                    'details': f"å·²å¤„ç† {i+1}/{len(documents)} ä¸ªæ–‡æ¡£"
                }
            )
        
        session.commit()
        
        success_count = len([r for r in results if r['status'] == 'success'])
        failed_count = len([r for r in results if r['status'] == 'failed'])
        skipped_count = len([r for r in results if r['status'] == 'skipped'])
        
        logger.info(f"é‡æ–°æå–å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}, è·³è¿‡ {skipped_count}")
        
        return {
            "status": "completed",
            "total_docs": len(documents),
            "success_count": success_count,
            "failed_count": failed_count,
            "skipped_count": skipped_count,
            "results": results[:20],  # åªè¿”å›å‰20ä¸ªç»“æœ
            "task_id": task_id
        }

# ==================== æ‰¹é‡å¤„ç†ä»»åŠ¡ ====================

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.batch_process_documents")
def batch_process_documents(self, file_paths: list[str], user_id: Optional[str] = None):
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼ˆåŒ…å«çŸ¥è¯†å›¾è°±æå–ï¼‰"""
    task_id = self.request.id
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£ï¼Œä»»åŠ¡ID: {task_id}")
    
    results = []
    for i, file_path in enumerate(file_paths):
        try:
            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æ–‡æ¡£ID
            doc_id = int(time.time() * 1000) + i  # ä½¿ç”¨æ—¶é—´æˆ³é¿å…å†²çª
            
            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå­ä»»åŠ¡
            sub_task = process_large_document.apply_async(
                args=[file_path, doc_id, user_id],
                queue="documents"
            )
            
            results.append({
                "file_path": file_path,
                "doc_id": doc_id,
                "task_id": sub_task.id,
                "status": "queued"
            })
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / len(file_paths) * 100
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': i + 1,
                    'total': len(file_paths),
                    'stage': 'ä»»åŠ¡åˆ†å‘ä¸­',
                    'progress': progress,
                    'details': f"å·²åˆ†å‘ {i+1}/{len(file_paths)} ä¸ªä»»åŠ¡"
                }
            )
            
        except Exception as e:
            logger.error(f"åˆ†å‘ä»»åŠ¡å¤±è´¥: {file_path} - {e}")
            results.append({
                "file_path": file_path,
                "error": str(e),
                "status": "failed"
            })
    
    return {
        "status": "completed",
        "total_tasks": len(file_paths),
        "results": results,
        "task_id": task_id
    }

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.cleanup_orphaned_entities")
def cleanup_orphaned_entities(self, user_id: Optional[str] = None):
    """æ¸…ç†å­¤ç«‹çš„å®ä½“ï¼ˆæ²¡æœ‰å…³è”æ–‡æ¡£çš„å®ä½“ï¼‰"""
    task_id = self.request.id
    logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†å­¤ç«‹å®ä½“ï¼Œä»»åŠ¡ID: {task_id}")
    
    with self.db_session as session:
        # è·å–æ‰€æœ‰å®ä½“
        if user_id:
            entities = session.exec(
                select(Entity).where(Entity.user_id == user_id)
            ).all()
        else:
            entities = session.exec(select(Entity)).all()
        
        logger.info(f"æ‰¾åˆ° {len(entities)} ä¸ªå®ä½“")
        
        orphaned_entities = []
        for i, entity in enumerate(entities):
            # æ£€æŸ¥å®ä½“æ˜¯å¦æœ‰å…³è”çš„æ–‡æ¡£
            if not entity.documents or len(entity.documents) == 0:
                # æ£€æŸ¥å®ä½“æ˜¯å¦æœ‰å…³ç³»
                if (not entity.outgoing_edges or len(entity.outgoing_edges) == 0) and \
                   (not entity.incoming_edges or len(entity.incoming_edges) == 0):
                    orphaned_entities.append(entity)
            
            # æ¯å¤„ç†100ä¸ªå®ä½“æ›´æ–°ä¸€æ¬¡è¿›åº¦
            if i % 100 == 0:
                progress = (i + 1) / len(entities) * 100
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': i + 1,
                        'total': len(entities),
                        'stage': 'æ‰«æä¸­',
                        'progress': progress,
                        'details': f"å·²æ‰«æ {i+1}/{len(entities)} ä¸ªå®ä½“ï¼Œæ‰¾åˆ° {len(orphaned_entities)} ä¸ªå­¤ç«‹å®ä½“"
                    }
                )
        
        # åˆ é™¤å­¤ç«‹å®ä½“
        deleted_count = 0
        for entity in orphaned_entities:
            try:
                session.delete(entity)
                deleted_count += 1
            except Exception as e:
                logger.error(f"åˆ é™¤å®ä½“å¤±è´¥ {entity.id}: {e}")
        
        session.commit()
        
        logger.info(f"æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªå­¤ç«‹å®ä½“")
        
        return {
            "status": "completed",
            "total_entities": len(entities),
            "orphaned_count": len(orphaned_entities),
            "deleted_count": deleted_count,
            "task_id": task_id
        }

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.cleanup_orphaned_edges")
def cleanup_orphaned_edges(self, user_id: Optional[str] = None):
    """æ¸…ç†å­¤ç«‹çš„å…³ç³»è¾¹ï¼ˆæºæˆ–ç›®æ ‡å®ä½“ä¸å­˜åœ¨çš„è¾¹ï¼‰"""
    task_id = self.request.id
    logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†å­¤ç«‹å…³ç³»è¾¹ï¼Œä»»åŠ¡ID: {task_id}")
    
    with self.db_session as session:
        # è·å–æ‰€æœ‰å…³ç³»è¾¹
        if user_id:
            edges = session.exec(
                select(GraphEdge).where(GraphEdge.user_id == user_id)
            ).all()
        else:
            edges = session.exec(select(GraphEdge)).all()
        
        logger.info(f"æ‰¾åˆ° {len(edges)} ä¸ªå…³ç³»è¾¹")
        
        orphaned_edges = []
        for i, edge in enumerate(edges):
            # æ£€æŸ¥æºå®ä½“å’Œç›®æ ‡å®ä½“æ˜¯å¦å­˜åœ¨
            source_exists = session.get(Entity, edge.source_id) is not None
            target_exists = session.get(Entity, edge.target_id) is not None
            
            if not source_exists or not target_exists:
                orphaned_edges.append(edge)
            
            # æ¯å¤„ç†100ä¸ªè¾¹æ›´æ–°ä¸€æ¬¡è¿›åº¦
            if i % 100 == 0:
                progress = (i + 1) / len(edges) * 100
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': i + 1,
                        'total': len(edges),
                        'stage': 'æ‰«æä¸­',
                        'progress': progress,
                        'details': f"å·²æ‰«æ {i+1}/{len(edges)} ä¸ªå…³ç³»è¾¹ï¼Œæ‰¾åˆ° {len(orphaned_edges)} ä¸ªå­¤ç«‹è¾¹"
                    }
                )
        
        # åˆ é™¤å­¤ç«‹è¾¹
        deleted_count = 0
        for edge in orphaned_edges:
            try:
                session.delete(edge)
                deleted_count += 1
            except Exception as e:
                logger.error(f"åˆ é™¤å…³ç³»è¾¹å¤±è´¥ {edge.id}: {e}")
        
        session.commit()
        
        logger.info(f"æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªå­¤ç«‹å…³ç³»è¾¹")
        
        return {
            "status": "completed",
            "total_edges": len(edges),
            "orphaned_count": len(orphaned_edges),
            "deleted_count": deleted_count,
            "task_id": task_id
        }