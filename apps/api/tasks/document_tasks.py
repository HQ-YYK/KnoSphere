import os
import time
import asyncio
from pathlib import Path
from typing import Dict, Any
from celery import Task
from sqlmodel import Session, select
from tasks.celery_app import celery_app
from core.logger import logger, WorkflowLogger
from services.embedding import generate_vector
from database import engine
from models import Document

class BaseTaskWithDB(Task):
    """å¸¦æœ‰æ•°æ®åº“è¿æ¥çš„åŸºç¡€ä»»åŠ¡ç±»"""
    
    def __init__(self):
        super().__init__()
        self.db_session = None
    
    def before_start(self, task_id, args, kwargs):
        """ä»»åŠ¡å¼€å§‹å‰åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        logger.info(f"ä»»åŠ¡ {task_id} å¼€å§‹æ‰§è¡Œ")
        self.db_session = Session(engine)
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥æ—¶çš„å¤„ç†"""
        logger.error(f"ä»»åŠ¡ {task_id} å¤±è´¥: {exc}", exc_info=True)
        if self.db_session:
            self.db_session.rollback()
    
    def after_return(self, status, retval, task_id, args, kwargs, einfo):
        """ä»»åŠ¡è¿”å›åçš„æ¸…ç†"""
        if self.db_session:
            self.db_session.close()
        logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆï¼ŒçŠ¶æ€: {status}")

@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.process_large_document")
def process_large_document(self, file_path: str, doc_id: int, user_id: int = None):
    """
    å¤„ç†å¤§æ–‡æ¡£ä»»åŠ¡
    
    å‚æ•°:
    - file_path: æ–‡ä»¶è·¯å¾„
    - doc_id: æ–‡æ¡£ID
    - user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ å¼€å§‹åå°å¤„ç†æ–‡æ¡£ ID: {doc_id}, ä»»åŠ¡ID: {task_id}")
    
    try:
        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # 2. æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
        file_ext = os.path.splitext(file_path)[1].lower()
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path} ({file_size:.2f}MB)")
        
        # 3. è¯»å–æ–‡ä»¶å†…å®¹
        content = ""
        total_steps = 0
        
        if file_ext == '.pdf':
            content, total_steps = _process_pdf(file_path)
        elif file_ext == '.docx':
            content, total_steps = _process_docx(file_path)
        elif file_ext in ['.txt', '.md']:
            content, total_steps = _process_text(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        # 4. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ25%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 1,
                'total': 4,
                'stage': 'æ–‡ä»¶è§£æå®Œæˆ',
                'progress': 25,
                'details': f"å·²è§£æ {len(content)} å­—ç¬¦"
            }
        )
        
        # 5. åˆ†å‰²æ–‡æ¡£ä¸ºå—ï¼ˆæ¨¡æ‹Ÿå¤§æ–‡æ¡£å¤„ç†ï¼‰
        chunks = _split_into_chunks(content, max_chunk_size=2000)
        logger.info(f"ğŸ“‘ æ–‡æ¡£åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
        
        # 6. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ50%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 2,
                'total': 4,
                'stage': 'æ–‡æ¡£åˆ†å‰²å®Œæˆ',
                'progress': 50,
                'details': f"å·²åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—"
            }
        )
        
        # 7. ä¸ºæ¯ä¸ªå—ç”Ÿæˆå‘é‡
        vectors = []
        
        for i, chunk in enumerate(chunks):
            # æ ¹æ®å—çš„å¤§å°åŠ¨æ€é€‰æ‹©ç»´åº¦
            chunk_len = len(chunk)
            if chunk_len > 1000:
                mode = "precise"
            elif chunk_len > 500:
                mode = "balanced"
            else:
                mode = "fast"
            
            # ç”Ÿæˆå‘é‡
            vector = asyncio.run(generate_vector(chunk, mode=mode))
            vectors.append(vector)
            
            # æ›´æ–°å­è¿›åº¦
            if i % max(1, len(chunks) // 10) == 0:
                self.update_state(
                    state='PROGRESS',
                    meta={
                        'current': 2 + (i / len(chunks)),
                        'total': 4,
                        'stage': 'å‘é‡ç”Ÿæˆä¸­',
                        'progress': 50 + (i / len(chunks)) * 25,
                        'details': f"å·²ç”Ÿæˆ {i+1}/{len(chunks)} ä¸ªå‘é‡"
                    }
                )
        
        # 8. æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆ75%ï¼‰
        self.update_state(
            state='PROGRESS',
            meta={
                'current': 3,
                'total': 4,
                'stage': 'å‘é‡ç”Ÿæˆå®Œæˆ',
                'progress': 75,
                'details': f"å·²ç”Ÿæˆ {len(vectors)} ä¸ªå‘é‡"
            }
        )
        
        # 9. å­˜å‚¨åˆ°æ•°æ®åº“
        file_name = os.path.basename(file_path)
        with self.db_session as session:
            # æ›´æ–°æˆ–åˆ›å»ºæ–‡æ¡£è®°å½•
            document = session.get(Document, doc_id)
            if not document:
                document = Document(
                    id=doc_id,
                    title=file_name,
                    content=content[:10000] + "..." if len(content) > 10000 else content,
                    embedding=vectors[0] if vectors else None,
                    user_id=user_id
                )
                session.add(document)
            else:
                document.title = file_name
                document.content = content[:10000] + "..." if len(content) > 10000 else content
                document.embedding = vectors[0] if vectors else None
            
            session.commit()
            
            # è®°å½•å‘é‡åŒ–ç»“æœ
            logger.info(f"âœ… æ–‡æ¡£ {doc_id} å‘é‡åŒ–å®Œæˆï¼Œå­˜å‚¨ {len(vectors)} ä¸ªå‘é‡")
            
            # 10. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.debug(f"ğŸ—‘ï¸ å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            # 11. æœ€ç»ˆè¿›åº¦æ›´æ–°
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': 4,
                    'total': 4,
                    'stage': 'å­˜å‚¨å®Œæˆ',
                    'progress': 100,
                    'details': f"æ–‡æ¡£å·²æˆåŠŸå­˜å‚¨ï¼ŒID: {doc_id}"
                }
            )
            
            return {
                "status": "completed",
                "doc_id": doc_id,
                "title": file_name,
                "chunks_count": len(chunks),
                "vectors_count": len(vectors),
                "content_length": len(content),
                "task_id": task_id
            }
    
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}", exc_info=True)
        raise self.retry(exc=e, countdown=60)

def _process_pdf(file_path: str) -> tuple[str, int]:
    """å¤„ç† PDF æ–‡ä»¶"""
    try:
        import PyPDF2
        
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            
            content = ""
            for page_num in range(total_pages):
                page = pdf_reader.pages[page_num]
                content += page.extract_text() + "\n\n"
                
                # æ¯å¤„ç†10é¡µè®°å½•ä¸€æ¬¡è¿›åº¦
                if page_num % 10 == 0:
                    logger.debug(f"ğŸ“„ å·²å¤„ç† {page_num+1}/{total_pages} é¡µ")
            
            logger.info(f"âœ… PDF è§£æå®Œæˆ: {total_pages} é¡µ")
            return content, total_pages
            
    except ImportError:
        raise ImportError("è¯·å®‰è£… PyPDF2: pip install PyPDF2")
    except Exception as e:
        raise Exception(f"PDF å¤„ç†å¤±è´¥: {e}")

def _process_docx(file_path: str) -> tuple[str, int]:
    """å¤„ç† DOCX æ–‡ä»¶"""
    try:
        import docx
        
        doc = docx.Document(file_path)
        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
        
        content = "\n".join(paragraphs)
        logger.info(f"âœ… DOCX è§£æå®Œæˆ: {len(paragraphs)} æ®µè½")
        
        return content, len(paragraphs)
        
    except ImportError:
        raise ImportError("è¯·å®‰è£… python-docx: pip install python-docx")
    except Exception as e:
        raise Exception(f"DOCX å¤„ç†å¤±è´¥: {e}")

def _process_text(file_path: str) -> tuple[str, int]:
    """å¤„ç†æ–‡æœ¬æ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
            
        lines = content.count('\n') + 1
        logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶è§£æå®Œæˆ: {lines} è¡Œ")
        
        return content, lines
        
    except UnicodeDecodeError:
        # å°è¯•å…¶ä»–ç¼–ç 
        try:
            with open(file_path, 'r', encoding='gbk') as file:
                content = file.read()
                
            lines = content.count('\n') + 1
            logger.info(f"âœ… æ–‡æœ¬æ–‡ä»¶è§£æå®Œæˆ (GBKç¼–ç ): {lines} è¡Œ")
            
            return content, lines
        except:
            raise Exception("æ–‡ä»¶ç¼–ç æ— æ³•è¯†åˆ«")
    except Exception as e:
        raise Exception(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

def _split_into_chunks(text: str, max_chunk_size: int = 2000, overlap: int = 200) -> list[str]:
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºé‡å çš„å—"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + max_chunk_size
        
        # å¦‚æœä¸åœ¨æ®µè½è¾¹ç•Œï¼Œå‘å‰æ‰¾åˆé€‚çš„è¾¹ç•Œ
        if end < len(text):
            # å°è¯•åœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
            paragraph_end = text.find('\n\n', start + int(max_chunk_size * 0.8))
            if paragraph_end != -1 and paragraph_end < end + 500:
                end = paragraph_end
        
        chunk = text[start:end]
        chunks.append(chunk)
        
        # é‡å æ»‘åŠ¨
        start = end - overlap
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if start >= len(text) - 100:
            break
    
    return chunks

# å…¶ä»–ä»»åŠ¡ï¼šæ‰¹é‡å‘é‡åŒ–
@celery_app.task(bind=True, base=BaseTaskWithDB, name="tasks.document_tasks.batch_process_documents")
def batch_process_documents(self, file_paths: list[str], user_id: int = None):
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡æ¡£"""
    task_id = self.request.id
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡æ¡£ï¼Œä»»åŠ¡ID: {task_id}")
    
    results = []
    for i, file_path in enumerate(file_paths):
        try:
            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå­ä»»åŠ¡
            sub_task = process_large_document.apply_async(
                args=[file_path, i + 1000, user_id],  # ä½¿ç”¨ä¸´æ—¶ID
                queue="documents"
            )
            
            results.append({
                "file_path": file_path,
                "task_id": sub_task.id,
                "status": "queued"
            })
            
            # æ›´æ–°è¿›åº¦
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': i + 1,
                    'total': len(file_paths),
                    'stage': 'ä»»åŠ¡åˆ†å‘ä¸­',
                    'progress': (i + 1) / len(file_paths) * 100,
                    'details': f"å·²åˆ†å‘ {i+1}/{len(file_paths)} ä¸ªä»»åŠ¡"
                }
            )
            
        except Exception as e:
            logger.error(f"åˆ†å‘ä»»åŠ¡å¤±è´¥: {file_path} - {e}")
            results.append({
                "file_path": file_path,
                "error": str(e),
                "status": "failed"
            })
    
    return {
        "status": "completed",
        "total_tasks": len(file_paths),
        "results": results,
        "task_id": task_id
    }